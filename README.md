# ğŸ•·ï¸ WebCrawler

Eine moderne, in Go geschriebene REST API zum Crawlen und Scrapen von Websites mit einer schÃ¶nen Web-OberflÃ¤che.

## âœ¨ Features

- **REST API**: VollstÃ¤ndige REST API fÃ¼r Website-Crawling
- **Batch Processing**: Mehrere Websites gleichzeitig crawlen
- **Website Statistiken**: Detaillierte Statistiken Ã¼ber Websites
- **Moderne UI**: SchÃ¶ne Web-OberflÃ¤che mit Tabs
- **Concurrency**: Parallele Verarbeitung mit Goroutines
- **Graceful Shutdown**: Sauberes Herunterfahren
- **Strukturiertes Logging**: JSON-basiertes Logging
- **Konfiguration**: Umgebungsvariablen und YAML-Konfiguration
- **CORS Support**: Cross-Origin Resource Sharing
- **Version Management**: Professionelles Version-Handling

## ğŸš€ Installation

### Voraussetzungen

- Go 1.21 oder hÃ¶her
- Git

### Installation

```bash
# Repository klonen
git clone https://github.com/dein-username/webcrawler.git
cd webcrawler

# Dependencies installieren
go mod tidy

# Anwendung starten
go run main.go
```

### Mit Docker

```bash
# Docker Image bauen
docker build -t webcrawler .

# Container starten
docker run -p 8080:8080 webcrawler
```

## ğŸ“– Verwendung

### Web Interface

Ã–ffne deinen Browser und gehe zu `http://localhost:8080`

### API Endpunkte

#### 1. Einzelne Website crawlen
```bash
curl -X POST http://localhost:8080/api/v1/scrape \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com"}'
```

#### 2. Mehrere Websites crawlen
```bash
curl -X POST http://localhost:8080/api/v1/scrape/batch \
  -H "Content-Type: application/json" \
  -d '{"urls": ["https://example1.com", "https://example2.com"]}'
```

#### 3. Website Statistiken
```bash
curl http://localhost:8080/api/v1/scrape/stats/https%3A//example.com
```

#### 4. Health Check
```bash
curl http://localhost:8080/health
```

## âš™ï¸ Konfiguration

### Umgebungsvariablen

```bash
export PORT=8080
export LOG_LEVEL=info
export TIMEOUT=30
```

### Konfigurationsdatei (config.yaml)

```yaml
port: 8080
log_level: info
timeout: 30
```

## ğŸ“Š API Response Format

### Crawled Data
```json
{
  "success": true,
  "data": {
    "url": "https://example.com",
    "title": "Example Domain",
    "description": "This domain is for use in illustrative examples...",
    "keywords": ["example", "domain"],
    "images": ["https://example.com/image.jpg"],
    "links": ["https://example.com/page1"],
    "text": "Extracted text content...",
    "meta_tags": {
      "description": "Example description",
      "keywords": "example, domain"
    },
    "status_code": 200,
    "scraped_at": "2024-01-01T12:00:00Z"
  }
}
```

### Website Statistics
```json
{
  "success": true,
  "data": {
    "url": "https://example.com",
    "title_length": 15,
    "text_length": 1024,
    "image_count": 5,
    "link_count": 10,
    "keyword_count": 3,
    "meta_count": 8,
    "status_code": 200,
    "scraped_at": "2024-01-01T12:00:00Z"
  }
}
```

## ğŸ—ï¸ Projektstruktur

```
webcrawler/
â”œâ”€â”€ main.go                 # Hauptanwendung
â”œâ”€â”€ go.mod                  # Go Module
â”œâ”€â”€ go.sum                  # Dependencies Checksum
â”œâ”€â”€ VERSION                 # Versionsdatei
â”œâ”€â”€ README.md              # Dokumentation
â”œâ”€â”€ Dockerfile             # Docker Konfiguration
â”œâ”€â”€ .gitignore             # Git Ignore
â”œâ”€â”€ config/
â”‚   â””â”€â”€ config.yaml        # Konfigurationsdatei
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html         # Web Interface
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ server.go      # HTTP Server & Routes
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ config.go      # Konfigurationsmanagement
â”‚   â”œâ”€â”€ logger/
â”‚   â”‚   â””â”€â”€ logger.go      # Strukturiertes Logging
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â””â”€â”€ scraper.go     # Web Crawling Logic
â”‚   â””â”€â”€ version/
â”‚       â””â”€â”€ version.go     # Version Management
â””â”€â”€ tests/
    â””â”€â”€ scraper_test.go    # Unit Tests
```

## ğŸ§ª Tests

```bash
# Alle Tests ausfÃ¼hren
go test ./...

# Tests mit Coverage
go test -cover ./...

# Spezifische Tests
go test ./internal/scraper
```

## ğŸ³ Docker

### Dockerfile
```dockerfile
FROM golang:1.21-alpine AS builder
WORKDIR /app
COPY . .
RUN go mod download
RUN go build -o main .

FROM alpine:latest
RUN apk --no-cache add ca-certificates
WORKDIR /root/
COPY --from=builder /app/main .
COPY --from=builder /app/templates ./templates
EXPOSE 8080
CMD ["./main"]
```

### Docker Compose
```yaml
version: '3.8'
services:
  webcrawler:
    build: .
    ports:
      - "8080:8080"
    environment:
      - PORT=8080
      - LOG_LEVEL=info
    volumes:
      - ./config:/app/config
```

## ğŸ”§ Entwicklung

### Lokale Entwicklung
```bash
# Dependencies installieren
go mod tidy

# Anwendung im Debug-Modus starten
LOG_LEVEL=debug go run main.go

# Tests ausfÃ¼hren
go test ./...
```

### Code Formatierung
```bash
# Code formatieren
go fmt ./...

# Linting
golangci-lint run
```

## ğŸ“ Lizenz

Dieses Projekt ist unter der MIT-Lizenz lizenziert. Siehe [LICENSE](LICENSE) fÃ¼r Details.

## ğŸ¤ Beitragen

1. Fork das Repository
2. Erstelle einen Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Committe deine Ã„nderungen (`git commit -m 'Add some AmazingFeature'`)
4. Push zum Branch (`git push origin feature/AmazingFeature`)
5. Ã–ffne einen Pull Request

## ğŸ“ Support

Bei Fragen oder Problemen erstelle bitte ein Issue auf GitHub.

## ğŸš€ Roadmap

- [ ] WebSocket Support fÃ¼r Live Updates
- [ ] Rate Limiting
- [ ] Authentication & Authorization
- [ ] Database Integration
- [ ] Caching Layer
- [ ] More Crawling Options
- [ ] Export to CSV/JSON
- [ ] Scheduled Crawling
- [ ] Sitemap Generation
- [ ] SEO Analysis

---

**Entwickelt mit â¤ï¸ in Go** 